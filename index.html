<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Training Phases – TTS Thesis</title>
  <base href="/Master-TTS-audio-site/">

  <style>
    body { font-family: Arial, sans-serif; margin: 40px; line-height: 1.6; }
    h1, h2, h3 { color: #2c3e50; }
    nav ul { list-style-type: none; padding: 0; }
    nav li { margin-bottom: 5px; }
    nav ul ul { padding-left: 20px; }
    section { margin-top: 50px; }
    audio { margin-top: 10px; display: block; }
  </style>
</head>
<body>

  <h1>Training Phases – Norwegian Text-to-Speech with StyleTTS 2</h1>

  <p>This site presents training results from a master's thesis titled <strong>"Norwegian-based Text-to-Speech, using StyleTTS 2"</strong>. Below is a summary of different configurations and training phases, each with audio samples for evaluation.</p>

  <nav>
    <h2>Table of Contents</h2>
    <ul>
      <li><a href="#nosyll">Without syllable break</a>
        <ul>
          <li><a href="#phase1-nosyll">1.1 Training Phase 1 </a></li>
          <li><a href="#phase2-nosyll">1.2 Training Phase 2</a></li>
          <li><a href="#inference-nosyll">1.3 Inference</a></li>
        </ul>
      </li>
      <li><a href="#syll">With syllable break</a>
         <ul>
          <li><a href="#phase1-syll">2.1 Training Phase 1 </a></li>
          <li><a href="#phase2-syll">2.2 Training Phase 2</a></li>
          <li><a href="#inference-syll">2.3 Inference</a></li>
        </ul>
      </li>
       <li><a href="#xLSTM">With xLSTM instead of LSTM</a>
         <ul>
          <li><a href="#phase1-xlstm">3.1 Training Phase 1 </a></li>
          <li><a href="#phase2-xlstm">3.2 Training Phase 2</a></li>
          <li><a href="#inference-xlstm">3.3 Inference</a></li>
        </ul>
      </li>
       <li><a href="#POS">With Part-of-Speech and Dependency tagging</a>
         <ul>
          <li><a href="#phase2-pos">4.1 Training Phase 2</a></li>
          <li><a href="#inference-pos">4.2 Inference</a></li>
        </ul>
      </li>
      <li><a href="#inf">Inferences used during MOS Collection</a>
      </li>
      <li><a href="#ood">Inferences on New Text (Out of Distribution)</a>
      </li>
    </ul>
  </nav>
<!-- -------------------------------------------------------------------------------- -->
  <section id="nosyll">
    <h2>Without syllable break</h2>

    <h3 id="phase1-nosyll">1.1 Training Phase 1</h3>
    <p>Figure 1. Model trained without syllable segmentation, stage 1, step 162 657, during epoch 10.</p>
    <audio controls>
      <source src="0syllable_reconstruct_Stage1_middle.wav" type="audio/wav">
    </audio>

    <p>Figure 2. Model trained without syllable segmentation, stage 1, step 272 995, during epoch 25.</p>

     <audio controls>
      <source src="0syllable_reconstruct_Stage1_last_new.wav" type="audio/wav">
    </audio>

    <h3 id="phase2-nosyll">1.2 Training Phase 2</h3>
    <p>Figure 3. Model trained without syllable segmentation, stage 2, step 193 087, during epoch 29.</p>
    <audio controls>
      <source src="0syllable_reconstruct_Stage2.wav" type="audio/wav">
    </audio>

    <p>Figure 4. Ground Truth to audio from Figure 3., stage 2, step 193 087, during epoch 29.</p>
     <audio controls>
      <source src="0syllable_reconstruct_Stage2_gt.wav" type="audio/wav">
    </audio>
    
    <h3 id="inference-nosyll">1.3 Inference</h3>
    <p>Figure 5. Model inferenced without syllable segmentation.</p>
    <audio controls>
      <source src="eval/ood/1_1.wav" type="audio/wav">
    </audio>
  </section>
<!-- -------------------------------------------------------------------------------- -->
  <section id="syll">
    <h2>With syllable break</h2>

    <h3 id="phase1-syll">2.1 Training Phase 1</h3>
    <p>Figure 6. Model trained with syllable segmentation, stage 1, step 120 876, during epoch 12.</p>
    <audio controls>
      <source src="1syllable_reconstruct_Stage1_last.wav" type="audio/wav">
    </audio>

    <h3 id="phase2-syll">2.2 Training Phase 2</h3>
    <p>Figure 7. Model trained with syllable segmentation, stage 2, step 238 756, during epoch 30.</p>
    <audio controls>
      <source src="1syllable_reconstruct_Stage2_last.wav" type="audio/wav">
    </audio>

    <p>Figure 8. Ground Truth to audio from Figure 7., stage 2, step 238 756, during epoch 30.</p>
     <audio controls>
      <source src="1syllable_reconstruct_Stage2_last_gt.wav" type="audio/wav">
    </audio>

    <h3 id="inference-syll">2.3 Inference</h3>
    <p>Figure 9. Model inferenced with syllable segmentation.</p>
    <audio controls>
      <source src="eval/ood/2_1.wav" type="audio/wav">
    </audio>

  </section>
  <!-- -------------------------------------------------------------------------------- -->
<section id="xLSTM">
    <h2>With xLSTM instead of LSTM</h2>

    <h3 id="phase1-xlstm">3.1 Training Phase 1</h3>
    <p>Figure 10. Model trained with syllable segmentation and xLSTM instead of LSTM, stage 1, step 219 165, during epoch 20.</p>
    <audio controls>
      <source src="1syllable_reconstruct_Stage1_xLSTM.wav" type="audio/wav">
    </audio>

    <h3 id="phase2-xlstm">3.2 Training Phase 2</h3>
    <p>Figure 11. Model trained with syllable segmentation and xLSTM instead of LSTM, stage 2, step 161 614, during epoch 20. (different bucket sizes than phase 1)</p>
    <audio controls>
      <source src="1syllable_reconstruct_Stage2_xLSTM.wav" type="audio/wav">
    </audio>

    <p>Figure 12. Ground Truth to audio from Figure 11., stage 2, step 161 614, during epoch 20.</p>
     <audio controls>
      <source src="1syllable_reconstruct_Stage2_POS_xLSTM_gt.wav" type="audio/wav">
    </audio>

    <h3 id="inference-xlstm">3.3 Inference</h3>
    <p>Figure 13. Model inferenced with syllable segmentation and xLSTM instead of LSTM.</p>
    <audio controls>
      <source src="eval/ood/3_1.wav" type="audio/wav">
    </audio>

  </section>

  <!-- -------------------------------------------------------------------------------- -->

<section id="POS">
    <h2>With Part-of-Speech and Dependency tagging</h2>

    <h3 id="phase2-pos">4.1 Training Phase 2</h3>
    <p>Figure 14. Model trained with syllable segmentation and xLSTM instead of LSTM, and with POS and DP tagging, stage 2, step 161 614, during epoch 20. (different bucket sizes than phase 1)</p>
    <audio controls>
      <source src="1syllable_reconstruct_Stage2_POS.wav" type="audio/wav">
    </audio>

    <p>Figure 15. Ground Truth to audio from Figure 14., stage 2, step 161 614, during epoch 20.</p>
     <audio controls>
      <source src="1syllable_reconstruct_Stage2_POS_xLSTM_gt.wav" type="audio/wav">
    </audio>

    <h3 id="inference-pos">4.2 Inference</h3>
    <p>Figure 16. Model inferenced with syllable segmentation and xLSTM instead of LSTM, and with POS and DP tagging.</p>
    <audio controls>
      <source src="eval/ood/4_4.wav" type="audio/wav">
    </audio>

  </section>

  <!-- -------------------------------------------------------------------------------- -->

  <!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Training Phases – TTS Thesis</title>
  <base href="/Master-TTS-audio-site/">

  <style>
    body { font-family: Arial, sans-serif; margin: 40px; line-height: 1.6; }
    h1, h2, h3 { color: #2c3e50; }
    nav ul { list-style-type: none; padding: 0; }
    nav li { margin-bottom: 5px; }
    nav ul ul { padding-left: 20px; }
    section { margin-top: 50px; }
    audio { margin-top: 10px; display: block; }

    /* ─────────────── New Table Styles ─────────────── */
    .table-wrapper { overflow-x: auto; margin-top: 1rem; }
    .audio-comparison {
      width: 100%;
      border-collapse: collapse;
      margin-top: 1rem;
    }
    .audio-comparison th,
    .audio-comparison td {
      padding: 0.75rem;
      text-align: center;
      border-bottom: 1px solid #ddd;
      vertical-align: middle;
    }
    .audio-comparison thead th {
      background-color: #f9f9f9;
      color: #2c3e50;
      font-weight: 600;
    }
    .visually-hidden {
      position: absolute;
      width: 1px; height: 1px;
      padding: 0; margin: -1px;
      overflow: hidden; clip: rect(0,0,0,0);
      white-space: nowrap; border: 0;
    }
    @media (max-width: 600px) {
      .audio-comparison thead { display: none; }
      .audio-comparison, 
      .audio-comparison tbody, 
      .audio-comparison tr, 
      .audio-comparison td {
        display: block; width: 100%;
      }
      .audio-comparison tr { margin-bottom: 1.5rem; }
      .audio-comparison td {
        text-align: right; padding-left: 50%; position: relative;
      }
      .audio-comparison td::before {
        content: attr(data-label);
        position: absolute; left: 0; width: 45%;
        padding-left: 0.75rem; font-weight: 600; text-align: left;
      }
    }
    /* ───────────────────────────────────────────────── */
  </style>
</head>
<body>


  <!-- ─────────────── In-distribution Inference ─────────────── -->
  <section id="inf">
    <h2>Inferences</h2>
    <p>This section contains audio samples from our models trained on the NPSC dataset.</p>
    <div class="table-wrapper">
      <table class="audio-comparison">
        <caption class="visually-hidden">
          Each row shows one text prompt (first column) and the corresponding audio from each model.
        </caption>
        <thead></thead>
        <tbody></tbody>
      </table>
    </div>
  </section>

  <!-- ─────────────── OOD Inference ─────────────── -->
  <section id="ood">
    <h2>Out-of-distribution Inference</h2>
    <p>This section contains audio samples from our models on unseen (OOD) data.</p>
    <div class="table-wrapper">
      <table class="audio-comparison">
        <caption class="visually-hidden">
          Each row shows one OOD text prompt (first column) and the corresponding audio from each model.
        </caption>
        <thead></thead>
        <tbody></tbody>
      </table>
    </div>
  </section>

  <script>
    document.addEventListener("DOMContentLoaded", () => {
      // Helper: build one table
      function buildTable(sectionId, texts, models) {
        // derive the directory of this page (e.g. "/Master-TTS-audio-site/")
        const path = window.location.pathname;
        const baseDir = path.endsWith("/")
          ? path
          : path.slice(0, path.lastIndexOf("/") + 1);

        // header
        const thead = document.querySelector(`#${sectionId} thead`);
        const headerRow = document.createElement("tr");
        headerRow.innerHTML = `<th>Text</th>` +
          models.map(m => `<th>${m.name}</th>`).join("");
        thead.append(headerRow);

        // body
        const tbody = document.querySelector(`#${sectionId} tbody`);
        texts.forEach((txt, i) => {
          const tr = document.createElement("tr");
          const cells = models.map(m => `
            <td>
              <audio controls>
                <source src="${baseDir}${m.paths[i]}" type="audio/wav">
              </audio>
            </td>
          `).join("");
          tr.innerHTML = `<th scope="row">${txt}</th>${cells}`;
          tbody.append(tr);
        });
      }

      // 1) In‐distribution ("inf") data
      const infTexts = [
        "Stortinget ber også regjeringa om å utrede, hvordan datostemplinga på matvarer kan forbedres for å redusere matsvinn.",
        "det vil alltid være forbundet med risiko å velge en strategi for veien ut av en konflikt.",
        "ee. ungdomsledigheten er en ee utfordring ee. men så vet vi at vi trenger flere fagarbeidere i framtida, og da er det viktig at vi også har nok lærlingplasser.",
        "vi har fremma forslag om å begrense bemanningsselskapenes handlingsrom i henhold til loven.",
        "heldige, fordi det er mulig å få til en myk landing, men krevende, fordi det er vanskelig å være innovativ og se etter nye muligheter samtidig som det går godt."
      ];
      const infModels = [
        {
          name: "ground-truth",
          paths: [
            "eval/v1/gt/47.wav",
            "eval/v1/gt/650.wav",
            "eval/v1/gt/2601.wav",
            "eval/v1/gt/37005.wav",
            "eval/v1/gt/29021.wav"
          ]
        },
        {
          name: "without syllable break",
          paths: [
            "eval/v1/pred/47.wav",
            "eval/v1/pred/650.wav",
            "eval/v1/pred/2601.wav",
            "eval/v1/pred/37005.wav",
            "eval/v1/pred/29021.wav"
          ]
        },
        {
          name: "with syllable break",
          paths: [
            "eval/v2/pred/47.wav",
            "eval/v2/pred/650.wav",
            "eval/v2/pred/2601.wav",
            "eval/v2/pred/37005.wav",
            "eval/v2/pred/29021.wav"
          ]
        },
        {
          name: "syllable break with xLSTM",
          paths: [
            "eval/v3/pred/47.wav",
            "eval/v3/pred/650.wav",
            "eval/v3/pred/2601.wav",
            "eval/v3/pred/37005.wav",
            "eval/v3/pred/29021.wav"
          ]
        },
        {
          name: "syllable break with PoS & UD",
          paths: [
            "eval/v4/pred/47.wav",
            "eval/v4/pred/650.wav",
            "eval/v4/pred/2601.wav",
            "eval/v4/pred/37005.wav",
            "eval/v4/pred/29021.wav"
          ]
        }
      ];
      buildTable("inf", infTexts, infModels);

      // 2) Out‐of‐distribution ("ood") data
      const oodTexts = [
        "Oskar og Lars, har utviklet en ny energidrikk ved hjelp av kunstig intelligens. Drikken, kalt Data brus, er resultatet av avanserte algoritmer som analyserte tusenvis av smaksprofiler for å finne den optimale kombinasjonen av ingredienser.",
        "Spania legger bak seg sine tre varmeste år noensinne, og er akkurat på vei ut av en årelang tørkeperiode. Klimaforskere viser til at menneskeskapte klimaendringer gjør at hyppigheten, varigheten og intensiteten av ekstremvær som hetebølger øker. -VG"

        
      ];
      const oodModels = [
        {
          name: "Reference audio ",
          paths: ["eval/v1/gt/29021.wav",
                  "eval/ood/45ref.wav"
                 
                 ]
        },
        {
          name: "without syllable break",
          paths: ["eval/ood/1.wav",
                  "eval/ood/1_1.wav"
                 
                 ]
        },
        {
          name: "with syllable break",
          paths: ["eval/ood/2.wav",
                 "eval/ood/2_1.wav"]
        },
        {
          name: "syllable break with xLSTM",
          paths: ["eval/ood/3.wav",
                 "eval/ood/3_1.wav"]
        },
        {
          name: "syllable break with PoS & UD",
          paths: ["eval/ood/4.wav",
                 "eval/ood/4_4.wav"]
        }
      ];
      buildTable("ood", oodTexts, oodModels);
    });
  </script>
</body>
</html>
